{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymysql\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "NUMBER_OF_POSTS =  # 1000\n",
    "LOG_FILE = \"C:\\\\Users\\\\ <REMOVED> \\\\joblog.txt\"\n",
    "SPACER = \"\\n\\n\\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \"\n",
    "SPACER2 = \"\\n====================================================\"\n",
    "PASSWORD = \"<REMOVED>\" \n",
    "MATCHDATA_FILE = \"C:\\\\Users\\\\ <REMOVED> \\\\matchData.txt\"\n",
    "\n",
    "try:\n",
    "    conn = pymysql.connect(host=\"<REMOVED>\", port=3306, user=\"<REMOVED>\", passwd=PASSWORD, db=\"<REMOVED>\")\n",
    "    cur = conn.cursor()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "pageLink1 = \"https://www.google.com/search?vet=<REMOVED>&client=firefox-b-1-d&yv=3&rciv=jb&nfpr=0&q=\"\n",
    "pageLink2 = \"&start=\"\n",
    "pageLink3 = \"&asearch=jb_list&async=_id:VoQFxe,_pms:hts,_fmt:pc\"\n",
    "\n",
    "titles = [\"software+engineer\", \"data+analyst\", \"web+developer\", \"database+developer\"]\n",
    "\n",
    "\n",
    "def search(TITLE):\n",
    "    global output\n",
    "    output = \"\"  # compiles all results from multiple soups bc we are using XHRs to infinitely scroll\n",
    "    counter = 0\n",
    "    while counter < NUMBER_OF_POSTS:\n",
    "        counter += 10\n",
    "        URL = pageLink1 + TITLE + pageLink2 + str(counter) + pageLink3\n",
    "        print(\"URL: \", URL)\n",
    "        hdr = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36'}\n",
    "        req = Request(URL, headers=hdr)\n",
    "        html = urlopen(req)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        output = output+str(soup)\n",
    "        time.sleep(.025)\n",
    "\n",
    "\n",
    "def getLinks():\n",
    "    global dataSet, matchData\n",
    "    txtloc = re.finditer(\"(?<=line-height:1.5em\\\">).+?(?=<\\/span><\\/)\", str(output), re.DOTALL)\n",
    "    link = re.finditer('(?<=href=\")([^\\'\" >]+)(?=\" jsaction=\"trigger)', str(output))\n",
    "    matchData = []  # list with text start position and description\n",
    "    urlData = []  # list with link start position and url\n",
    "    dataSet = []  # list of lists, each of which hold links to index-corresponding description\n",
    "    for words in txtloc:\n",
    "        matchData.append([words.start(), words[0]])\n",
    "    for i in link:\n",
    "        urlData.append([i.start(), i[0]])\n",
    "    for text in matchData:\n",
    "        block = []\n",
    "        dataSet.append(block)\n",
    "        for url in urlData:\n",
    "            if url[0] <= text[0] and url[1] != \"#\":\n",
    "                block.append(url[1])\n",
    "                url[0] = 999999999999999\n",
    "\n",
    "\n",
    "def getText():\n",
    "    global descrpt, clean_out\n",
    "    clean_out = []\n",
    "    descrpt = re.findall('(?<=line-height:1.5em\">).+?(?=<\\/span><\\/)', str(output), flags=re.DOTALL)\n",
    "    for i in range(len(descrpt)):\n",
    "        if re.search('(</span></div>)', str(descrpt[i]), flags=re.DOTALL):\n",
    "            clean_out.append(re.findall('.*(?=</span></div><div)', descrpt[i], flags=re.DOTALL))\n",
    "        else:\n",
    "            clean_out.append(re.findall('.*', str(descrpt[i]), flags=re.DOTALL))\n",
    "    for i in range(len(clean_out)):\n",
    "        clean_out[i] = re.sub(r'(<span)(.*)(\\\">)', ' ', str(clean_out[i][0]))\n",
    "\n",
    "\n",
    "oldSet = []\n",
    "indexCounter = 1\n",
    "sqlIDcounter = 1\n",
    "\n",
    "\n",
    "def sqlURL(TITLE, indexCounter):\n",
    "    global sqlIDcounter\n",
    "    for eachURL in dataSet:\n",
    "        if eachURL[0] not in oldSet:\n",
    "            cur.execute(\"insert into jobposting (url,category) values ( %s,%s)\", (str(eachURL[0]), str(TITLE)))\n",
    "            # print(cur.fetchall())\n",
    "            conn.commit()\n",
    "            oldSet.append(eachURL[0])\n",
    "            sqlSKILLS(indexCounter)\n",
    "            sqlIDcounter += 1\n",
    "            indexCounter += 1\n",
    "\n",
    "\n",
    "def sqlSKILLS(indexCounter):\n",
    "    try:\n",
    "        pattern = re.compile(r'python{0,5}', flags=re.IGNORECASE)\n",
    "        matches = pattern.findall(matchData[indexCounter-1][1])\n",
    "        for match in matches:\n",
    "            cur.execute(\"insert into kw_intersect (jobPosting_id, keywords_id) values (%s,%s)\", (sqlIDcounter, 1))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pattern = re.compile(r'sql{0,5}', flags=re.IGNORECASE)\n",
    "        matches = pattern.findall(matchData[indexCounter-1][1])\n",
    "        for match in matches:\n",
    "            cur.execute(\"insert into kw_intersect (jobPosting_id, keywords_id) values (%s,%s)\", (sqlIDcounter, 2))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pattern = re.compile(r'\\bexcel\\b', flags=re.IGNORECASE)\n",
    "        matches = pattern.findall(matchData[indexCounter-1][1])\n",
    "        for match in matches:\n",
    "            cur.execute(\"insert into kw_intersect (jobPosting_id, keywords_id) values (%s,%s)\", (sqlIDcounter, 3))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pattern = re.compile(r'C\\+\\+', flags=re.IGNORECASE)\n",
    "        matches = pattern.findall(matchData[indexCounter-1][1])\n",
    "        for match in matches:\n",
    "            cur.execute(\"insert into kw_intersect (jobPosting_id, keywords_id) values (%s,%s)\", (sqlIDcounter, 4))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pattern = re.compile(r'\\bjava\\b', flags=re.IGNORECASE)\n",
    "        matches = pattern.findall(matchData[indexCounter-1][1])\n",
    "        for match in matches:\n",
    "            cur.execute(\"insert into kw_intersect (jobPosting_id, keywords_id) values (%s,%s)\", (sqlIDcounter, 5))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pattern = re.compile(r'\\bgithub\\b', flags=re.IGNORECASE)\n",
    "        matches = pattern.findall(matchData[indexCounter-1][1])\n",
    "        for match in matches:\n",
    "            cur.execute(\"insert into kw_intersect (jobPosting_id, keywords_id) values (%s,%s)\", (sqlIDcounter, 6))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pattern = re.compile(r'\\php\\b', flags=re.IGNORECASE)\n",
    "        matches = pattern.findall(matchData[indexCounter-1][1])\n",
    "        for match in matches:\n",
    "            cur.execute(\"insert into kw_intersect (jobPosting_id, keywords_id) values (%s,%s)\", (sqlIDcounter, 7))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pattern = re.compile(r'\\bjavascript\\b', flags=re.IGNORECASE)\n",
    "        matches = pattern.findall(matchData[indexCounter-1][1])\n",
    "        for match in matches:\n",
    "            cur.execute(\"insert into kw_intersect (jobPosting_id, keywords_id) values (%s,%s)\", (sqlIDcounter, 8))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def logOutput():\n",
    "    with open(LOG_FILE, 'a') as f:\n",
    "        for i in range(len(descrpt)):\n",
    "            try:\n",
    "                print(SPACER, file=f)\n",
    "                print(TITLE, file=f)\n",
    "                print(\"JOB NUMBER: \", i+1, file=f)\n",
    "                print(clean_out[i], file=f)\n",
    "                print(SPACER2, file=f)\n",
    "                print(dataSet[i], file=f)\n",
    "            except:\n",
    "                pass\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def logMatchData(TITLE):\n",
    "    with open(MATCHDATA_FILE, \"a\") as f:\n",
    "        for item in matchData:\n",
    "            try:\n",
    "                f.write('%s\\n' % item)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for TITLE in titles:\n",
    "        search(TITLE)\n",
    "        getLinks()\n",
    "        getText()\n",
    "        logOutput()\n",
    "        logMatchData(TITLE)\n",
    "        sqlURL(TITLE, indexCounter)\n",
    "        print(\"TOTAL NUMBER OF ITEMS: \", len(clean_out))\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
